import os
from omegaconf import OmegaConf
import numpy as np
from einops import rearrange, repeat

import torch
import torch.nn as nn
import torch.nn.functional as F
import wandb

from torchvision.utils import make_grid
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from diffusers import StableUnCLIPImg2ImgPipeline
from diffusers.utils import load_image

from sc_mbm.mae_for_fmri import MAEforFMRICross
from sc_mbm.mae_for_image import ViTMAEConfig, ViTMAELayer


def create_fmri_mae(config, sd, config_pretrain, model_image_config, device):
    num_voxels = (sd["model"]["pos_embed"].shape[1] - 1) * config_pretrain.patch_size
    model = MAEforFMRICross(
        num_voxels=num_voxels,
        patch_size=config_pretrain.patch_size,
        embed_dim=config_pretrain.embed_dim,
        decoder_embed_dim=config_pretrain.decoder_embed_dim,
        depth=config_pretrain.depth,
        num_heads=config_pretrain.num_heads,
        decoder_num_heads=config_pretrain.decoder_num_heads,
        mlp_ratio=config_pretrain.mlp_ratio,
        focus_range=None,
        use_nature_img_loss=False,
        # do_cross_attention=config.do_cross_attention,
        do_cross_attention=False,
        cross_encoder_config=model_image_config,
        decoder_depth=config.fmri_decoder_layers,
    )
    model.load_state_dict(sd["model"], strict=False)
    model.to(device)

    return model, num_voxels


class unCLIP(nn.Module):
    def __init__(self, model_image_config, clip_dim=1024, device=torch.device("cpu")):
        super().__init__()

        self.device = device

        self.pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1-unclip", torch_dtype=torch.float16
        )
        self.pipe.enable_vae_slicing()
        self.pipe.enable_xformers_memory_efficient_attention()
        self.pipe.set_progress_bar_config(disable=True)
        self.pipe.to(device)

        self.cross_blocks = nn.ModuleList(
            [
                ViTMAELayer(model_image_config, True)
                for _ in range(model_image_config.num_cross_encoder_layers)
            ]
        )
        self.norm = nn.LayerNorm(clip_dim)

    def encode_clip(self, img):
        image_features = self.pipe.feature_extractor(
            img, return_tensors="pt"
        ).pixel_values.to(self.device)
        image_embeddings = self.pipe.image_encoder(image_features)
        return image_embeddings.image_embeds

    def decode_clip(self, embeddings):
        images = self.pipe(image_embeds=embeddings, output_type="pt").images
        return transforms.Resize(256)(images)

    def forward(self, img, encoder_only=False, fmri_support=None):
        embeddings = self.encode_clip(img)
        if encoder_only:
            return embeddings
        else:
            x = embeddings
            cross_x = x.clone()

            for blk in self.cross_blocks:
                cross_x_full = blk(cross_x, hidden_states_mod2=fmri_support)
                cross_x = cross_x_full[0]

            x = x + cross_x

            x = self.norm(x)

            pred = self.decode_clip(x)
            return pred


class fMRICLIPAutoEncoder(nn.Module):
    def __init__(
        self, config, model_image_config, clip_dim=1024, device=torch.device("cpu")
    ):
        super().__init__()

        sd = torch.load(config.pretrain_mbm_path, map_location="cpu")
        config_pretrain = sd["config"]

        self.mae, self.num_voxels = create_fmri_mae(
            config, sd, config_pretrain, model_image_config, device
        )

        # Freeze Model so we're only learning linear layers
        for param in self.mae.parameters():
            param.requires_grad = False

        self.map_dims = nn.Conv1d(292, 1, 1)
        self.unmap_dims = nn.ConvTranspose1d(1, 292, 1)
        self.encoder = nn.Linear(config_pretrain.embed_dim, clip_dim)
        self.decoder = nn.Linear(config_pretrain.embed_dim, clip_dim)
        self.cross_blocks = nn.ModuleList(
            [
                ViTMAELayer(model_image_config, True)
                for _ in range(model_image_config.num_cross_encoder_layers)
            ]
        )
        self.norm = nn.LayerNorm(clip_dim)

    def encode_fmri(self, sample, mask_ratio=0.0):
        x, mask, ids_restore = self.mae.forward_encoder(sample, mask_ratio=mask_ratio)
        return x, (mask, ids_restore)

    def decode_fmri(self, x, metadata):
        _, ids_restore = metadata
        return self.mae.forward_decoder(x, ids_restore)

    def reconstruction_loss(self, target, pred, metadata):
        mask, _ = metadata
        return self.mae.forward_loss(target, pred, mask)

    def forward(self, sample, encoder_only=False, image_support=None):
        latent, metadata = self.encode_fmri(sample)
        print(latent.shape, self.map_dims(latent).shape)
        latent = self.map_dims(latent)
        x = self.encoder(latent)
        print(x.shape)
        if encoder_only:
            x = self.norm(x)
            return x
        else:
            image_support_2d = image_support.unsqueeze(1)
            print(f"{image_support.shape=}, {image_support_2d.shape=}")
            cross_x = x.clone()

            for blk in self.cross_blocks:
                print(f"{cross_x.shape=}")
                cross_x_full = blk(cross_x, hidden_states_mod2=image_support_2d)
                cross_x = cross_x_full[0]

            x = x + cross_x

            x = self.norm(x)
            # print(f"{x.shape=}, {self.unmap_dims(x).shape=}")
            x = self.unmap_dims(x)
            latent = self.decoder(x)
            pred = self.decode_fmri(latent, metadata)

            return pred, metadata
